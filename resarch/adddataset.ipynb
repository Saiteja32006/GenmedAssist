{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f27c90e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!Hello, World!!\n"
     ]
    }
   ],
   "source": [
    "print(\"!!Hello, World!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abbf232c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4.1\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "print(datasets.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc460480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f37b4d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD set to D:\\Sai Teja Honours\\GenmedAssist\n"
     ]
    }
   ],
   "source": [
    "# Complete Medical RAG System - GenmedAssist\n",
    "# Integrates PDF documents and Hugging Face symptom_to_diagnosis dataset\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Pinecone imports (keep as you used; adjust if your Pinecone client differs)\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Hugging Face datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ===========================\n",
    "# CONFIGURATION\n",
    "# ===========================\n",
    "\n",
    "# Set working directory\n",
    "WORKING_DIR = r\"D:\\Sai Teja Honours\\GenmedAssist\"\n",
    "os.chdir(WORKING_DIR)\n",
    "print(f'CWD set to {os.getcwd()}')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# fail early if env vars missing\n",
    "if not PINECONE_API_KEY:\n",
    "    raise EnvironmentError(\"PINECONE_API_KEY not set. Please add it to your .env file or environment.\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY not set. Please add it to your .env file or environment.\")\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# Index configuration\n",
    "INDEX_NAME = \"genmedassist\"\n",
    "EMBEDDING_DIMENSION = 384\n",
    "LOAD_NEW_DATA = False  # Set to True to upload new documents\n",
    "\n",
    "# (Continue with the rest of your pipeline here: loading PDFs, creating embeddings,\n",
    "# creating/connecting to Pinecone index, loading HF dataset etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f33e27c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===========================\n",
    "# STEP 1: LOAD PDF DOCUMENTS\n",
    "# ===========================\n",
    "\n",
    "def load_pdf_files(data_dir: str) -> List[Document]:\n",
    "    \"\"\"Load all PDF files from the specified directory.\"\"\"\n",
    "    pdf_files = Path(data_dir).glob(\"*.pdf\")\n",
    "    documents = []\n",
    "    for pdf in pdf_files:\n",
    "        loader = PyPDFLoader(str(pdf))\n",
    "        documents.extend(loader.load())\n",
    "    print(f\"‚úÖ Loaded {len(documents)} pages from PDF files\")\n",
    "    return documents\n",
    "\n",
    "def filter_to_minimal_docs(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"Filter documents to minimal metadata (only source).\"\"\"\n",
    "    minimal_docs = []\n",
    "    for doc in documents:\n",
    "        src = doc.metadata.get(\"source\") if isinstance(doc.metadata, dict) else None\n",
    "        minimal_docs.append(Document(page_content=doc.page_content, metadata={\"source\": src}))\n",
    "    return minimal_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93800b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# STEP 2: LOAD HUGGING FACE DATASET\n",
    "# ===========================\n",
    "\n",
    "def load_symptom_diagnosis_dataset() -> List[Document]:\n",
    "    \"\"\"Load the Gretel AI symptom_to_diagnosis dataset and convert to Documents.\"\"\"\n",
    "    print(\"\\nüì• Loading Hugging Face dataset: gretelai/symptom_to_diagnosis\")\n",
    "    \n",
    "    # Load dataset\n",
    "    ds = load_dataset(\"gretelai/symptom_to_diagnosis\")\n",
    "    \n",
    "    # Convert to LangChain Documents\n",
    "    documents = []\n",
    "    \n",
    "    # Process train split\n",
    "    if 'train' in ds:\n",
    "        for idx, example in enumerate(ds['train']):\n",
    "            # Create a comprehensive text from the dataset fields\n",
    "            content = f\"\"\"\n",
    "Patient Case {idx + 1}:\n",
    "\n",
    "Symptoms: {example.get('Patient_Symptoms', 'N/A')}\n",
    "Diagnosis: {example.get('Diagnosis', 'N/A')}\n",
    "\n",
    "Additional Information:\n",
    "- Age: {example.get('Age', 'N/A')}\n",
    "- Gender: {example.get('Gender', 'N/A')}\n",
    "- Disease: {example.get('Disease', 'N/A')}\n",
    "- Fever: {example.get('Fever', 'N/A')}\n",
    "- Cough: {example.get('Cough', 'N/A')}\n",
    "- Fatigue: {example.get('Fatigue', 'N/A')}\n",
    "- Difficulty Breathing: {example.get('Difficulty Breathing', 'N/A')}\n",
    "- Blood Pressure: {example.get('Blood Pressure', 'N/A')}\n",
    "- Cholesterol Level: {example.get('Cholesterol Level', 'N/A')}\n",
    "\"\"\".strip()\n",
    "            \n",
    "            doc = Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    \"source\": \"huggingface_symptom_diagnosis\",\n",
    "                    \"dataset_index\": idx,\n",
    "                    \"diagnosis\": example.get('Diagnosis', ''),\n",
    "                    \"disease\": example.get('Disease', '')\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(documents)} cases from Hugging Face dataset\")\n",
    "    return documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc3d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# STEP 3: TEXT PROCESSING\n",
    "# ===========================\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Remove unwanted characters and normalize spacing.\"\"\"\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def text_split(documents: List[Document], chunk_size: int = 500, chunk_overlap: int = 20) -> List[Document]:\n",
    "    \"\"\"Clean and split documents into smaller chunks.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "    )\n",
    "    \n",
    "    # Clean documents\n",
    "    cleaned_docs = []\n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        cleaned_docs.append(doc)\n",
    "    \n",
    "    # Split documents\n",
    "    texts_chunk = text_splitter.split_documents(cleaned_docs)\n",
    "    \n",
    "    # Clean chunks again\n",
    "    for chunk in texts_chunk:\n",
    "        chunk.page_content = clean_text(chunk.page_content)\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(texts_chunk)} text chunks\")\n",
    "    return texts_chunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79d4c755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# STEP 4: EMBEDDINGS\n",
    "# ===========================\n",
    "\n",
    "def download_embeddings():\n",
    "    \"\"\"Download and return the HuggingFace embeddings model.\"\"\"\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    print(f\"‚úÖ Loaded embedding model: {model_name}\")\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15b454f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# STEP 5: PINECONE SETUP\n",
    "# ===========================\n",
    "\n",
    "def setup_pinecone_index(texts_chunk: List[Document], embedding, load_new: bool = False):\n",
    "    \"\"\"Create or connect to Pinecone index.\"\"\"\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    INDEX_NAME = \"genmedassist\"\n",
    "    # Create index if it doesn't exist\n",
    "    if not pc.has_index(INDEX_NAME):\n",
    "        print(f\"üîß Creating new index: {INDEX_NAME}\")\n",
    "        pc.create_index(\n",
    "            name=INDEX_NAME,\n",
    "            dimension=EMBEDDING_DIMENSION,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "        )\n",
    "    else:\n",
    "        print(f\"‚úÖ Index '{INDEX_NAME}' already exists\")\n",
    "    \n",
    "    # Load or upload documents\n",
    "    if load_new:\n",
    "        print(\"‚¨ÜÔ∏è Uploading new documents to Pinecone...\")\n",
    "        docsearch = PineconeVectorStore.from_documents(\n",
    "            documents=texts_chunk,\n",
    "            embedding=embedding,\n",
    "            index_name=INDEX_NAME,\n",
    "        )\n",
    "        print(\"‚úÖ Uploaded new documents to Pinecone\")\n",
    "    else:\n",
    "        print(\"üîí Loading existing Pinecone index (no upload)\")\n",
    "        docsearch = PineconeVectorStore.from_existing_index(\n",
    "            index_name=INDEX_NAME,\n",
    "            embedding=embedding,\n",
    "        )\n",
    "    \n",
    "    return docsearch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5d21e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# STEP 6: RAG CHAIN SETUP\n",
    "# ===========================\n",
    "\n",
    "def create_rag_chain(docsearch):\n",
    "    \"\"\"Create the RAG chain with retriever and LLM.\"\"\"\n",
    "    # Create retriever\n",
    "    retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "    \n",
    "    # Initialize ChatGPT model\n",
    "    chatModel = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "    \n",
    "    # Define system prompt\n",
    "    system_prompt = (\n",
    "        \"You are a Medical assistant for diagnostic tasks that gives concise and accurate information. \"\n",
    "        \"You will give diagnoses based on the context provided. For patients, use simple terms. \"\n",
    "        \"For doctors, provide technical terms using ICD-10 codes when available. \"\n",
    "        \"Use the following pieces of retrieved context to answer the question. \"\n",
    "        \"If you don't know the answer, say that you don't know. \"\n",
    "        \"Use three sentences maximum and keep the answer concise.\\n\\n\"\n",
    "        \"{context}\"\n",
    "    )\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ])\n",
    "    \n",
    "    # Create chains\n",
    "    question_answer_chain = create_stuff_documents_chain(chatModel, prompt)\n",
    "    rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "    \n",
    "    print(\"‚úÖ RAG chain created successfully\")\n",
    "    return rag_chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af5acb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# MAIN EXECUTION\n",
    "# ===========================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üè• MEDICAL RAG SYSTEM - GenmedAssist\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Step 1: Load PDF documents\n",
    "    print(\"üìö Step 1: Loading PDF documents...\")\n",
    "    extracted_data = load_pdf_files(\"data\")\n",
    "    minimal_docs = filter_to_minimal_docs(extracted_data)\n",
    "    \n",
    "    # Step 2: Load Hugging Face dataset\n",
    "    print(\"\\nüìä Step 2: Loading Hugging Face dataset...\")\n",
    "    hf_documents = load_symptom_diagnosis_dataset()\n",
    "    \n",
    "    # Combine all documents\n",
    "    all_documents = minimal_docs + hf_documents\n",
    "    print(f\"\\n‚úÖ Total documents: {len(all_documents)}\")\n",
    "    print(f\"   - PDF documents: {len(minimal_docs)}\")\n",
    "    print(f\"   - HF dataset cases: {len(hf_documents)}\")\n",
    "    \n",
    "    # Step 3: Split documents\n",
    "    print(\"\\nüìù Step 3: Splitting documents into chunks...\")\n",
    "    texts_chunk = text_split(all_documents)\n",
    "    \n",
    "    # Step 4: Load embeddings\n",
    "    print(\"\\nüßÆ Step 4: Loading embeddings model...\")\n",
    "    embedding = download_embeddings()\n",
    "    \n",
    "    # Step 5: Setup Pinecone\n",
    "    print(\"\\nüîß Step 5: Setting up Pinecone...\")\n",
    "    docsearch = setup_pinecone_index(texts_chunk, embedding, load_new=LOAD_NEW_DATA)\n",
    "    \n",
    "    # Step 6: Create RAG chain\n",
    "    print(\"\\nü§ñ Step 6: Creating RAG chain...\")\n",
    "    rag_chain = create_rag_chain(docsearch)\n",
    "    \n",
    "    # Test queries\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üß™ TESTING THE SYSTEM\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    test_queries = [\n",
    "        \"What is Acne? Symptoms and treatment options?\",\n",
    "        \"What are the symptoms of diabetes?\",\n",
    "        \"How to treat high blood pressure?\",\n",
    "        \"What causes fever and cough together?\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\n‚ùì Query: {query}\")\n",
    "        print(\"-\" * 60)\n",
    "        response = rag_chain.invoke({\"input\": query})\n",
    "        print(f\"üí° Answer: {response['answer']}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"‚úÖ System ready for use!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return rag_chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42377688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üè• MEDICAL RAG SYSTEM - GenmedAssist\n",
      "============================================================\n",
      "\n",
      "üìö Step 1: Loading PDF documents...\n",
      "‚úÖ Loaded 637 pages from PDF files\n",
      "\n",
      "üìä Step 2: Loading Hugging Face dataset...\n",
      "\n",
      "üì• Loading Hugging Face dataset: gretelai/symptom_to_diagnosis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 853/853 [00:00<00:00, 132102.84 examples/s]\n",
      "Generating test split:   0%|          | 0/212 [00:00<?, ? examples/s]"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# RUN THE SYSTEM\n",
    "# ===========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rag_chain = main()\n",
    "    \n",
    "    # Interactive mode\n",
    "    print(\"\\nüéØ Enter your medical questions (type 'quit' to exit):\")\n",
    "    while True:\n",
    "        user_input = input(\"\\n‚ùì Your question: \")\n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"üëã Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        response = rag_chain.invoke({\"input\": user_input})\n",
    "        print(f\"\\nüí° Answer: {response['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1477d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2451ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6775ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546fe8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenmedAssist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
