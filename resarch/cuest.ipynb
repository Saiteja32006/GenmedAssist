{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello, World!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b58769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set working directory to project root so relative paths resolve consistently\n",
    "os.chdir(r\"D:\\Sai Teja Honours\\GenmedAssist\")\n",
    "print('CWD set to', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b80e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f628943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c45f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_pdf_files(data):\n",
    "    pdf_files = Path(data).glob(\"*.pdf\")\n",
    "    documents = []\n",
    "    for pdf in pdf_files:\n",
    "        loader = PyPDFLoader(str(pdf))\n",
    "        documents.extend(loader.load())\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600c680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf_files(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d41b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433da66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "def filter_to_minimal_docs(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Given a list of Document objects, return a new list of Document objects\n",
    "    containing only the original page_content and a metadata dict with 'source'.\n",
    "    \"\"\"\n",
    "    minimal_docs: List[Document] = []\n",
    "    for doc in documents:\n",
    "        src = doc.metadata.get(\"source\") if isinstance(doc.metadata, dict) else None\n",
    "        minimal_docs.append(Document(page_content=doc.page_content, metadata={\"source\": src}))\n",
    "    return minimal_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f19d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_docs = filter_to_minimal_docs(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaeb146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "# --- Clean Function ---\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove unwanted characters and normalize spacing.\"\"\"\n",
    "    text = text.replace('\\n', ' ')               # Replace newlines with spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()     # Remove multiple spaces\n",
    "    return text\n",
    "\n",
    "# --- Split + Clean Function ---\n",
    "def text_split(minimal_docs):\n",
    "    \"\"\"Clean and split documents into smaller chunks.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=20,\n",
    "    )\n",
    "\n",
    "    cleaned_docs = []\n",
    "    for doc in minimal_docs:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        cleaned_docs.append(doc)\n",
    "\n",
    "    # Split the cleaned docs\n",
    "    texts_chunk = text_splitter.split_documents(cleaned_docs)\n",
    "\n",
    "    # Clean each chunk again (safety pass)\n",
    "    for chunk in texts_chunk:\n",
    "        chunk.page_content = clean_text(chunk.page_content)\n",
    "\n",
    "    # Print all cleaned chunks before embedding\n",
    "    print(\"\\nðŸ“„ --- Cleaned Text Chunks ---\\n\")\n",
    "    for i, chunk in enumerate(texts_chunk, 1):\n",
    "        print(f\"Chunk {i}:\")\n",
    "        print(chunk.page_content)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    return texts_chunkfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "# --- Clean Function ---\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove unwanted characters and normalize spacing.\"\"\"\n",
    "    text = text.replace('\\n', ' ')               # Replace newlines with spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()     # Remove multiple spaces\n",
    "    return text\n",
    "\n",
    "# --- Split + Clean Function ---\n",
    "def text_split(minimal_docs):\n",
    "    \"\"\"Clean and split documents into smaller chunks.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=20,\n",
    "    )\n",
    "\n",
    "    cleaned_docs = []\n",
    "    for doc in minimal_docs:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        cleaned_docs.append(doc)\n",
    "\n",
    "    # Split the cleaned docs\n",
    "    texts_chunk = text_splitter.split_documents(cleaned_docs)\n",
    "\n",
    "    # Clean each chunk again (safety pass)\n",
    "    for chunk in texts_chunk:\n",
    "        chunk.page_content = clean_text(chunk.page_content)\n",
    "\n",
    "    # Print all cleaned chunks before embedding\n",
    "    print(\"\\nðŸ“„ --- Cleaned Text Chunks ---\\n\")\n",
    "    for i, chunk in enumerate(texts_chunk, 1):\n",
    "        print(f\"Chunk {i}:\")\n",
    "        print(chunk.page_content)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    return texts_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ffc3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_chunk = text_split(minimal_docs)\n",
    "print(f\"Number of chunks: {len(texts_chunk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77016dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab366f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def download_embeddings():\n",
    "    \"\"\"\n",
    "    Download and return the HuggingFace embeddings model.\n",
    "    \"\"\"\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "embedding = download_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b413439",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = embedding.embed_query(\"Hello world\")\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe7c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Vector length:\", len(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print( \"Vector length:\", len(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758273c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c41bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751d42bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pinecone import Pinecone \n",
    "pinecone_api_key = PINECONE_API_KEY\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcebf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ccdc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec \n",
    "\n",
    "index_name = \"genmedassist\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name = index_name,\n",
    "        dimension=384,  # Dimension of the embeddings\n",
    "        metric= \"cosine\",  # Cosine similarity\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b2274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=texts_chunk,\n",
    "    embedding=embedding,\n",
    "    index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d47a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing index \n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e10bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ques = retriever.invoke(\"what are white blood cells ?\")\n",
    "ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae784a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dbaf84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f54d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5db1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c408b75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb45332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a164f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f9a489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6f263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c6fd03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e49214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a64c24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ad4c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963bd599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e2387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e937f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd084d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d10f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenmedAssist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
